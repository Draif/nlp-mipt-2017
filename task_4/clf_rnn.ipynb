{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "## Павел Яковенко, гр. 295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"avito_train.tsv\",sep='\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3995803, 13) 0.0688212106553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000025</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Монтаж кровли</td>\n",
       "      <td>Выполняем  монтаж кровли фальцевой ^p Тел:8@@P...</td>\n",
       "      <td>{\"Вид услуги\":\"Ремонт, строительство\"}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000101</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Ford Focus, 2011</td>\n",
       "      <td>Автомобиль в отличном техническом состоянии, в...</td>\n",
       "      <td>{\"Марка\":\"Ford\", \"Модель\":\"Focus\", \"Год выпуск...</td>\n",
       "      <td>365000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000132</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>Турбина 3.0 Bar</td>\n",
       "      <td>Продам турбину на двигатель V-6 . V-8 и мощнее...</td>\n",
       "      <td>{\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid     category                subcategory              title  \\\n",
       "0  10000010    Транспорт      Автомобили с пробегом  Toyota Sera, 1991   \n",
       "1  10000025       Услуги          Предложения услуг      Монтаж кровли   \n",
       "2  10000094  Личные вещи  Одежда, обувь, аксессуары   Костюм Steilmann   \n",
       "3  10000101    Транспорт      Автомобили с пробегом   Ford Focus, 2011   \n",
       "4  10000132    Транспорт      Запчасти и аксессуары    Турбина 3.0 Bar   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Выполняем  монтаж кровли фальцевой ^p Тел:8@@P...   \n",
       "2  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "3  Автомобиль в отличном техническом состоянии, в...   \n",
       "4  Продам турбину на двигатель V-6 . V-8 и мощнее...   \n",
       "\n",
       "                                               attrs   price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000        NaN   \n",
       "1             {\"Вид услуги\":\"Ремонт, строительство\"}       0        NaN   \n",
       "2  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...    1500        NaN   \n",
       "3  {\"Марка\":\"Ford\", \"Модель\":\"Focus\", \"Год выпуск...  365000        NaN   \n",
       "4  {\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...    5000        NaN   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           1           0         0        22.38  \n",
       "2           0           0           0         0         0.41  \n",
       "3           0           0           0         0         8.87  \n",
       "4           0           0           0         0        11.82  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.0688212106553\n",
      "Count: 3995803\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\", df.is_blocked.mean()\n",
    "print \"Count:\", len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 549992\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "MAX_NOT_BLOCKED_ADS = len(df[df.is_blocked == True])\n",
    "\n",
    "df_filtered = pd.concat([df[df.is_blocked == True], df[df.is_blocked == False][:MAX_NOT_BLOCKED_ADS]])\n",
    "\n",
    "\n",
    "print \"Blocked ratio:\", df_filtered.is_blocked.mean()\n",
    "print \"Count:\", len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df_filtered.is_blocked.mean() < 0.51\n",
    "assert df_filtered.is_blocked.mean() > 0.49\n",
    "assert len(df_filtered) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df_filtered.description.values, df_filtered.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7fbf83250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "tokens = [token for token, occur in token_counts.iteritems() if occur >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t: i + 1 for i, t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 87950\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\", len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df_filtered.description.values, token_to_id, max_len = 150)\n",
    "title_tokens = vectorize(df_filtered.title.values, token_to_id, max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [43275 14651 55417 82139 80223 17327     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 8340     0 30473     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [28819 23424     0  3628 33973     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print u\"Размер матрицы:\", title_tokens.shape\n",
    "for title, tokens in zip(df_filtered.title.values[:3], title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df_filtered[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df_filtered[[\"category\", \"subcategory\"]].values\n",
    "\n",
    "categories = [\n",
    "    {\"category\": category, \"subcategory\": subcategory} \n",
    "    for category, subcategory in data_cat_subcat\n",
    "]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot, columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features, cat_one_hot, on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df_filtered.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert target.shape[0] == title_tokens.shape[0] == desc_tokens.shape[0] == df_non_text.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я разбил на test/train в отношение 1/3, причем сделал это сбалансированно - и в обучающей, и в тестируемой выборке одинаковое количество плохих и хороших объявлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FACTOR = 0.75\n",
    "SIZE = len(target)\n",
    "TRAIN_SIZE_PER_ANSWER = int(FACTOR * SIZE / 2)\n",
    "train_indexes_not_blocked = []\n",
    "train_indexes_blocked = []\n",
    "test_indexes_blocked = []\n",
    "test_indexes_not_blocked = []\n",
    "for i in xrange(SIZE):\n",
    "    if target[i]:\n",
    "        if len(train_indexes_blocked) < TRAIN_SIZE_PER_ANSWER:\n",
    "            train_indexes_blocked.append(i)\n",
    "        else:\n",
    "            test_indexes_blocked.append(i)\n",
    "    else:\n",
    "        if len(train_indexes_not_blocked) < TRAIN_SIZE_PER_ANSWER:\n",
    "            train_indexes_not_blocked.append(i)\n",
    "        else:\n",
    "            test_indexes_not_blocked.append(i)\n",
    "            \n",
    "train_indexes = train_indexes_blocked + train_indexes_not_blocked\n",
    "test_indexes = test_indexes_blocked + test_indexes_not_blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(test_indexes) + len(train_indexes) == SIZE\n",
    "assert len(train_indexes) == int(SIZE * FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_tr, title_ts = title_tokens[train_indexes], title_tokens[test_indexes]\n",
    "desc_tr, desc_ts = desc_tokens[train_indexes], desc_tokens[test_indexes] \n",
    "nontext_tr, nontext_ts = df_non_text.as_matrix()[train_indexes], df_non_text.as_matrix()[test_indexes]\n",
    "target_tr, target_ts = target[train_indexes], target[test_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    data_tuple = title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple, fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id, fout)\n",
    "\n",
    "    print \"done\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 2: Tesla K40m (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я не стал использовать категориальные и численные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\", dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\", dtype='int32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None, title_tr.shape[1]), input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None, desc_tr.shape[1]), input_var=desc_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто один embedding и над ним RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id) + 1, output_size=128)\n",
    "descr_nn = lasagne.layers.RecurrentLayer(descr_nn, num_units=512)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id) + 1, output_size=128)\n",
    "title_nn = lasagne.layers.RecurrentLayer(title_nn, num_units=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединяем слои, добавляем нормализатор и дропаут. Выход - один ответ, вероятность объявлению \"быть плохим\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.ConcatLayer([descr_nn, title_nn]) \n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 4096)\n",
    "nn = lasagne.layers.BatchNormLayer(nn)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.5)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "Использовал hinge loss с стандартными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:, 0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, log_odds=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.nesterov_momentum(loss, weights, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:, 0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, log_odds=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids, title_token_ids, target_y],[loss, prediction], updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,target_y],[det_loss, det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(*arrays, **kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Границу разделения верояности(вероятность \"быть плохим\" больше границы -- объявление плохое) задал равным 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.573809485431\n",
      "\tacc: 0.747920792079\n",
      "\tauc: 0.843282146167\n",
      "\tap@k: 0.999376391166\n",
      "Validation:\n",
      "\tloss: 0.360201027401\n",
      "\tacc: 0.849405940594\n",
      "\tauc: 0.929378770386\n",
      "\tap@k: 1.0\n",
      "Epoch 1 of 5 took 44.991s\n",
      "Train:\n",
      "\tloss: 0.308707235431\n",
      "\tacc: 0.87297029703\n",
      "\tauc: 0.947780470389\n",
      "\tap@k: 1.0\n",
      "Validation:\n",
      "\tloss: 0.277203640908\n",
      "\tacc: 0.889306930693\n",
      "\tauc: 0.959505402049\n",
      "\tap@k: 0.990610402034\n",
      "Epoch 2 of 5 took 44.940s\n",
      "Train:\n",
      "\tloss: 0.259469711594\n",
      "\tacc: 0.895940594059\n",
      "\tauc: 0.961258493952\n",
      "\tap@k: 0.996459332138\n",
      "Validation:\n",
      "\tloss: 0.288446915129\n",
      "\tacc: 0.884554455446\n",
      "\tauc: 0.955100216273\n",
      "\tap@k: 0.991629930339\n",
      "Epoch 3 of 5 took 44.826s\n",
      "Train:\n",
      "\tloss: 0.245325669244\n",
      "\tacc: 0.904257425743\n",
      "\tauc: 0.964001684698\n",
      "\tap@k: 0.988981492857\n",
      "Validation:\n",
      "\tloss: 0.225180011996\n",
      "\tacc: 0.913762376238\n",
      "\tauc: 0.970719303752\n",
      "\tap@k: 1.0\n",
      "Epoch 4 of 5 took 44.801s\n",
      "Train:\n",
      "\tloss: 0.206178736395\n",
      "\tacc: 0.919306930693\n",
      "\tauc: 0.974153742381\n",
      "\tap@k: 1.0\n",
      "Validation:\n",
      "\tloss: 0.234556042184\n",
      "\tacc: 0.911386138614\n",
      "\tauc: 0.969066277448\n",
      "\tap@k: 1.0\n",
      "Epoch 5 of 5 took 44.824s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "BORDER = 0.5\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    start_time = time.time()\n",
    "    for j, (b_desc, b_title, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr, title_tr, target_tr, batchsize=batch_size, shuffle=True)):\n",
    "        \n",
    "        if j > minibatches_per_epoch:\n",
    "            break\n",
    "            \n",
    "        loss, pred_probas = train_fun(b_desc, b_title, b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c += 1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    train_loss = b_loss / b_c\n",
    "    try:    \n",
    "        print \"Train:\"\n",
    "        print '\\tloss:', train_loss\n",
    "        print '\\tacc:', accuracy_score(epoch_y_true, epoch_y_pred > BORDER)\n",
    "        print '\\tauc:', roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "        print '\\tap@k:', APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    except Exception as e:\n",
    "        print e\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc, b_title, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts, title_ts, target_ts, batchsize=batch_size, shuffle=True)):\n",
    "        \n",
    "        if j > minibatches_per_epoch:\n",
    "            break\n",
    "        \n",
    "        loss, pred_probas = eval_fun(b_desc, b_title, b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c += 1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    val_loss = b_loss / b_c\n",
    "    \n",
    "    try:    \n",
    "        print \"Validation:\"\n",
    "        print '\\tloss:',b_loss/b_c\n",
    "        print '\\tacc:', accuracy_score(epoch_y_true, epoch_y_pred > BORDER)\n",
    "        print '\\tauc:', roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "        print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    except Exception as e:\n",
    "        print e\n",
    "    \n",
    "    \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        i + 1, n_epochs, time.time() - start_time))\n",
    "    \n",
    "    if train_loss < val_loss and abs(train_loss - val_loss) > 0.05 and i >= n_epochs / 2:\n",
    "        print \"Break by diff in losses\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "5 эпох было достаточно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.230350272295\n",
      "\tacc: 0.911397379913\n",
      "\tauc: 0.969878387086\n",
      "\tap@k: 0.996086964963\n",
      "\n",
      "AUC:\n",
      "\tСойдёт, хотя можно ещё поднажать (ok)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tЗасабмить на kaggle! (great) \n",
      "\t Нет, ну честно - выкачай avito_test.tsv, засабмить и скажи, что вышло.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "We Are Watching You!\n",
      "                                                      .             ..                           \n",
      "                                    .      ...;c:,::..  '  ':oococ:. ..                          \n",
      "                              ..    .':dodxkkxxxxxxxkxddxkkkkkkkkkxkkdl:.                        \n",
      "                             .'':lodxxxdxxxxxddxxddodddxkkxdxxxxxxxxxxxxk:d;,.                   \n",
      "                          .;dxkxxxxxxxxk000Okdooooooloodxxdddxxkkxxddddodkkkkd'  .               \n",
      "                       .:lkkkkkkkkkkkOOOkkxxxxdollllllloooddxxkOOOOkxdddldkkkOxc.                \n",
      "                     .lkkkOOOkkkkOOOkkxdollcccccccccccccccllddxkOOOOkddddddxxxddd;.              \n",
      "                   .cxkkOOOOOOOOkkkdolc:::;;;;;;;;;;;;;::::ccclodxkOOxddooodddooodo'             \n",
      "                  ,dxkkOOOOOOOOkxolc::;;;;;;;;;;;;;;;;;;;;::::cclodxkkxddooolooollooc..          \n",
      "                 ,xxkkkOOO0OOkxdol:;;;;;;,,,,;;;;;;;;;;;;;;;;;::ccloxxxxdoooollllollod,          \n",
      "                .xxkkkkOOOkxxollc;;;;,,,,,,,,,,,,,;,,;;;;;;;;;;::cclodxxddoollllcclllclc.        \n",
      "                ;xkkxkOOkdoolllc;;;,,,,,,,,,,,,,,;;;,,;;;;;;;;;:::clloddddoooolccc::lcclo:       \n",
      "               .ddkkxOOxdoolllc:;;;;;;;;;;;;,,,,,;;,,,;;;;;;;::::::ccodlcdddooolccc::cllll..     \n",
      "              'xdddxxOOkxolclc::;;:coooooollllc:;;,,;;;;::ccoddddddoollc;coxkxoc:::cc:clcco.     \n",
      "              oxddddxkxxdoc::::;:lolccccccccloolc;,,;:ccloddxxxkkOOOkdl::lxkkkxoc:;:cccccoo.     \n",
      "              xdododdkkxdlc::c::coc::::clooollllc:;;:cllldxxxddddddxkkdodxkkkxxkdl:;::::odc      \n",
      "             ,dddoodkkkdlc::cc::lccccldkkOOxdddol:;;:codkOO000OkdooodkkxxkOkxxkkkdlc:::;cd.      \n",
      "             :odoodkkxolc:ccc::cllloxkdollloooddoc;;:lxO00OkxxxxkxdddkkkxkkkxkO0Okxolc;;:c .     \n",
      "             oddodkOxolc:::c::::cloddo;;:c:;:llc::,,;lxOOko:;:cccdxxkOkkkkkxkkOO00kdllc;:o..     \n",
      "             cddddkxdlc:::c::;;::cc:cc;:clccc::;;,,,;:oxkxc;:cloloxkkkkkxxkxdxollkxl;:cc:c;.     \n",
      "             .oddxkkdc::cc::;;;;;;;;;cllllllcc:;,,,,;codxkdllooddxkxddddddxkooc::dOd:;:lolc'     \n",
      "              'dddxxolcllcc:;;,,,,;;;;;;;:;;;;;;,,,,;:loddoccllodddlccclodxkxl:::dOdl::codlc     \n",
      "               cxdddooodolo:;;,,,,,,,,,,,,,,,;;;,',,;:codol:;::::c::;:clodxxko::cdkolc;ccolc.    \n",
      "                 cdxxxdkdodl:;,,,,,,,''''',::ll:;,;:llodddoc;;,;;;;::cclloxkkxlcokxolc:lcll:.    \n",
      "                  ldoddkxodl::;,,,,,,,',;:c::ooccldxkO0K0kxxo:;,;;;:ccllclxkkxlcdkxol::dol:      \n",
      "                  .dddoddoxc::;;,;;;,,;:cc;;;;;;:lxkkkkkkkkkko;;;;:cloooccxkxxocxkooocc;cd.      \n",
      "                   ,cldddl:;;:;;;;;;;;:c:;,,,,,,,;;:cllodddxddc:::clooddlcxOxoclxkooool  :       \n",
      "                     'xdd:;;;::;;;;;;;::;,,,,,,,,;;;;;:cccllllcc:cclooddl:xkxoloxxdodoc          \n",
      "                  ..  cdxoccc::;;;;;;;;;;;;::cccc:ccllloooodolccc:clloodccxxdoodxxdodc .         \n",
      "                       .;;  .::;;;;;;;;;:lxO0kdollodxxdkkOOOkdlcc;:cclodclodoodxdl:,.            \n",
      "                            .c::;;;;;,;ccodOxlc:codkOOOOO0KK0xc:;;:cllddc,.do;.',.  .            \n",
      "                           .clc::;;;;;;;;;::::::cclcllodxddooc,;;:clldxo:..;,                    \n",
      "                         .;clllcc:::;;,,,,;;;;;;;::cccclodddl;;;:looxkxl.                        \n",
      "                        .c:ccooclcc::;;,,,;;;:::ccllooddddol:::coxxkkkc .  .                     \n",
      "                        :l:clodccllcc:;;;;,;;;;:::cccccc:::cclodkkkkx:                           \n",
      "                       ;ll:clddcccloolc:;;,,,,;,,,,;;:::::cldxkkkOkc                             \n",
      "                      .ddcclld,lcclloooolc;;;,,,;;;;:ccclldxxdc;,;.                              \n",
      "                      :docclod,'lccclloddxdollclllloooool:;.                                     \n",
      "                     .dxlcclldo :cccllloodxkd.   ....                                            \n",
      "                     ;dxlcclodx.'lcclllloodxx.                     \n",
      " \n",
      "\n",
      "                           ______________________________________\n",
      "                 _\\|/^    /       Молодцы, а теперь слезайте     \\    \\|| /\n",
      "                  (_oo   /              с казённой GPU            \\   oo   /  \n",
      "                   |     \\________________________________________/  О_    --\n",
      "                  /|\\                                                 )    =\n",
      "                   |                                                 (.   --\n",
      "                   LL                                                 1 1\\ \n",
      "                mborisyak@                                           jheuristic@\n",
      "    "
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc, b_title, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts, title_ts, target_ts, batchsize=batch_size, shuffle=True)):\n",
    "    loss, pred_probas = eval_fun(b_desc, b_title, b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true, epoch_y_pred > BORDER)\n",
    "final_auc = roc_auc_score(epoch_y_true, epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true, epoch_y_pred, K = int(len(epoch_y_pred)*0.025) + 1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
